{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.contrib import itertools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './small_dog_cat_dataset/train/'\n",
    "test_dir = './small_dog_cat_dataset/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "target_size = (64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,datas,labels):\n",
    "    for category in os.listdir(path):\n",
    "     category_dir = os.path.join(path, category)\n",
    "     for image_name in os.listdir(category_dir):\n",
    "        image_path = os.path.join(category_dir, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, target_size)\n",
    "        datas.append(image)\n",
    "        labels.append(category)\n",
    "    return np.array(datas), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[ 64,  64,  64],\n",
       "          [ 57,  57,  57],\n",
       "          [ 60,  60,  60],\n",
       "          ...,\n",
       "          [190, 190, 190],\n",
       "          [192, 192, 192],\n",
       "          [191, 191, 191]],\n",
       " \n",
       "         [[ 68,  68,  68],\n",
       "          [ 56,  56,  56],\n",
       "          [ 61,  61,  61],\n",
       "          ...,\n",
       "          [191, 191, 191],\n",
       "          [193, 193, 193],\n",
       "          [193, 193, 193]],\n",
       " \n",
       "         [[ 62,  62,  62],\n",
       "          [ 58,  58,  58],\n",
       "          [ 65,  65,  65],\n",
       "          ...,\n",
       "          [192, 192, 192],\n",
       "          [194, 194, 194],\n",
       "          [194, 194, 194]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[157, 162, 165],\n",
       "          [154, 158, 161],\n",
       "          [148, 151, 154],\n",
       "          ...,\n",
       "          [171, 174, 179],\n",
       "          [170, 173, 178],\n",
       "          [170, 173, 178]],\n",
       " \n",
       "         [[167, 169, 170],\n",
       "          [195, 197, 198],\n",
       "          [187, 189, 190],\n",
       "          ...,\n",
       "          [169, 173, 178],\n",
       "          [168, 171, 176],\n",
       "          [166, 169, 175]],\n",
       " \n",
       "         [[196, 198, 199],\n",
       "          [196, 198, 199],\n",
       "          [200, 202, 203],\n",
       "          ...,\n",
       "          [167, 171, 176],\n",
       "          [166, 169, 174],\n",
       "          [165, 168, 173]]],\n",
       " \n",
       " \n",
       "        [[[170, 174, 169],\n",
       "          [169, 173, 168],\n",
       "          [164, 173, 170],\n",
       "          ...,\n",
       "          [254, 245, 228],\n",
       "          [253, 251, 250],\n",
       "          [255, 251, 250]],\n",
       " \n",
       "         [[174, 178, 173],\n",
       "          [175, 179, 174],\n",
       "          [168, 175, 168],\n",
       "          ...,\n",
       "          [255, 248, 230],\n",
       "          [252, 250, 248],\n",
       "          [255, 252, 251]],\n",
       " \n",
       "         [[176, 180, 175],\n",
       "          [171, 174, 169],\n",
       "          [167, 172, 163],\n",
       "          ...,\n",
       "          [252, 242, 225],\n",
       "          [253, 252, 250],\n",
       "          [255, 252, 251]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[198, 193, 190],\n",
       "          [199, 194, 191],\n",
       "          [197, 196, 192],\n",
       "          ...,\n",
       "          [218, 213, 203],\n",
       "          [215, 210, 201],\n",
       "          [216, 211, 202]],\n",
       " \n",
       "         [[198, 193, 190],\n",
       "          [199, 194, 191],\n",
       "          [197, 196, 192],\n",
       "          ...,\n",
       "          [218, 213, 204],\n",
       "          [215, 210, 201],\n",
       "          [216, 211, 202]],\n",
       " \n",
       "         [[199, 194, 191],\n",
       "          [200, 196, 193],\n",
       "          [200, 197, 189],\n",
       "          ...,\n",
       "          [219, 214, 205],\n",
       "          [218, 213, 204],\n",
       "          [218, 213, 204]]],\n",
       " \n",
       " \n",
       "        [[[ 91,  99, 111],\n",
       "          [ 96, 106, 112],\n",
       "          [100, 108, 114],\n",
       "          ...,\n",
       "          [116, 122, 128],\n",
       "          [114, 122, 129],\n",
       "          [121, 125, 133]],\n",
       " \n",
       "         [[ 96, 104, 116],\n",
       "          [ 95, 105, 111],\n",
       "          [103, 113, 111],\n",
       "          ...,\n",
       "          [126, 132, 139],\n",
       "          [123, 132, 138],\n",
       "          [123, 127, 135]],\n",
       " \n",
       "         [[ 82,  90, 102],\n",
       "          [ 94, 105, 111],\n",
       "          [ 98, 105, 114],\n",
       "          ...,\n",
       "          [125, 131, 138],\n",
       "          [124, 131, 138],\n",
       "          [126, 131, 138]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 61,  60,  64],\n",
       "          [ 64,  62,  67],\n",
       "          [ 79,  81,  82],\n",
       "          ...,\n",
       "          [101, 112, 116],\n",
       "          [ 99, 110, 114],\n",
       "          [ 96, 106, 112]],\n",
       " \n",
       "         [[ 54,  54,  54],\n",
       "          [ 49,  49,  49],\n",
       "          [ 64,  59,  59],\n",
       "          ...,\n",
       "          [ 99, 110, 114],\n",
       "          [ 94, 104, 108],\n",
       "          [ 94, 104, 110]],\n",
       " \n",
       "         [[ 64,  59,  60],\n",
       "          [ 66,  65,  69],\n",
       "          [ 85,  81,  80],\n",
       "          ...,\n",
       "          [ 99, 108, 117],\n",
       "          [ 94, 105, 112],\n",
       "          [ 94, 104, 111]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[153, 143, 153],\n",
       "          [150, 145, 154],\n",
       "          [155, 156, 160],\n",
       "          ...,\n",
       "          [ 25,  37,  43],\n",
       "          [ 27,  39,  45],\n",
       "          [ 42,  50,  53]],\n",
       " \n",
       "         [[159, 148, 158],\n",
       "          [152, 147, 156],\n",
       "          [156, 155, 164],\n",
       "          ...,\n",
       "          [ 24,  36,  42],\n",
       "          [ 33,  46,  51],\n",
       "          [ 44,  52,  55]],\n",
       " \n",
       "         [[153, 142, 152],\n",
       "          [150, 146, 155],\n",
       "          [156, 155, 166],\n",
       "          ...,\n",
       "          [ 28,  40,  46],\n",
       "          [ 37,  49,  54],\n",
       "          [ 48,  56,  59]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 14,  41,  20],\n",
       "          [  8,  43,  21],\n",
       "          [  5,  42,  16],\n",
       "          ...,\n",
       "          [ 21,  33,  24],\n",
       "          [ 16,  31,  16],\n",
       "          [ 27,  51,  32]],\n",
       " \n",
       "         [[  5,  28,  10],\n",
       "          [  1,  28,   9],\n",
       "          [  6,  38,  11],\n",
       "          ...,\n",
       "          [ 33,  45,  33],\n",
       "          [  6,  32,  16],\n",
       "          [ 17,  34,  20]],\n",
       " \n",
       "         [[  8,  30,  12],\n",
       "          [  2,  30,  11],\n",
       "          [ 15,  47,  22],\n",
       "          ...,\n",
       "          [ 33,  53,  34],\n",
       "          [  1,  26,   7],\n",
       "          [ 18,  37,  26]]],\n",
       " \n",
       " \n",
       "        [[[  1,   1,   1],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  2,   8,  19],\n",
       "          [  0,   5,  16],\n",
       "          [  0,   5,  15]],\n",
       " \n",
       "         [[  1,   1,   1],\n",
       "          [  0,   0,   0],\n",
       "          [  3,   3,   3],\n",
       "          ...,\n",
       "          [  0,   3,  14],\n",
       "          [  1,   6,  17],\n",
       "          [  3,  11,  22]],\n",
       " \n",
       "         [[  1,   1,   1],\n",
       "          [  1,   1,   1],\n",
       "          [  1,   1,   1],\n",
       "          ...,\n",
       "          [  0,   5,  16],\n",
       "          [  0,   5,  16],\n",
       "          [  4,  12,  22]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  1,   1,   1],\n",
       "          [  1,   1,   1],\n",
       "          ...,\n",
       "          [130, 124, 127],\n",
       "          [147, 141, 142],\n",
       "          [146, 140, 141]],\n",
       " \n",
       "         [[  1,   1,   1],\n",
       "          [  1,   1,   1],\n",
       "          [  1,   1,   1],\n",
       "          ...,\n",
       "          [139, 134, 135],\n",
       "          [147, 141, 142],\n",
       "          [149, 141, 144]],\n",
       " \n",
       "         [[  1,   1,   1],\n",
       "          [  1,   1,   1],\n",
       "          [  1,   1,   1],\n",
       "          ...,\n",
       "          [140, 133, 134],\n",
       "          [136, 130, 131],\n",
       "          [135, 127, 128]]],\n",
       " \n",
       " \n",
       "        [[[ 76,  68,  68],\n",
       "          [ 90,  83,  83],\n",
       "          [ 80,  72,  72],\n",
       "          ...,\n",
       "          [194, 196, 197],\n",
       "          [194, 194, 200],\n",
       "          [192, 192, 198]],\n",
       " \n",
       "         [[ 81,  73,  73],\n",
       "          [ 87,  78,  78],\n",
       "          [ 83,  74,  74],\n",
       "          ...,\n",
       "          [194, 196, 197],\n",
       "          [192, 192, 198],\n",
       "          [190, 190, 196]],\n",
       " \n",
       "         [[ 70,  62,  62],\n",
       "          [ 81,  72,  72],\n",
       "          [ 64,  56,  56],\n",
       "          ...,\n",
       "          [193, 195, 196],\n",
       "          [192, 192, 198],\n",
       "          [190, 190, 196]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 24,  16,  16],\n",
       "          [ 41,  33,  33],\n",
       "          [ 47,  39,  39],\n",
       "          ...,\n",
       "          [ 66,  68,  69],\n",
       "          [ 63,  67,  62],\n",
       "          [ 69,  73,  68]],\n",
       " \n",
       "         [[ 26,  20,  21],\n",
       "          [ 28,  22,  23],\n",
       "          [ 40,  34,  35],\n",
       "          ...,\n",
       "          [ 62,  64,  65],\n",
       "          [ 72,  72,  72],\n",
       "          [ 72,  72,  72]],\n",
       " \n",
       "         [[ 28,  22,  23],\n",
       "          [ 24,  18,  19],\n",
       "          [ 37,  31,  32],\n",
       "          ...,\n",
       "          [ 69,  71,  72],\n",
       "          [ 72,  70,  74],\n",
       "          [ 72,  71,  75]]]], dtype=uint8),\n",
       " array(['cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats', 'cats',\n",
       "        'cats', 'cats', 'cats', 'cats', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs',\n",
       "        'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs', 'dogs'],\n",
       "       dtype='<U4'))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file(train_dir,train_data,train_labels)\n",
    "read_file(test_dir,test_data,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorspace(data):\n",
    "   hsv_images = []\n",
    "   for image in data:\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        hsv_images.append(hsv)\n",
    "   return np.array(hsv_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hsv_images = colorspace(train_data)\n",
    "test_hsv_images = colorspace(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = LabelEncoder().fit_transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = LabelEncoder().fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOGhsvfearture(data):\n",
    "    hog_hsv_features =[]\n",
    "    for image in data:\n",
    "        # hue = image[:,:,0]\n",
    "        hog_features, hog_image = hog(image, visualize=True,\n",
    "                                       block_norm='L2-Hys',\n",
    "                                         pixels_per_cell=(16, 16),\n",
    "                                           cells_per_block=(2, 2),\n",
    "                                           channel_axis=-1)\n",
    "        hog_hsv_features.append(hog_features)\n",
    "    return hog_hsv_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hsv_feature = HOGhsvfearture(train_hsv_images)\n",
    "test_hsv_feature = HOGhsvfearture(test_hsv_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hsv_feature = np.array(train_hsv_feature)\n",
    "test_hsv_feature = np.array(test_hsv_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    \"\"\"\n",
    "    Computes the logistic function applied to an input scalar/array\n",
    "    Args:\n",
    "        x (scalar/ndarray): scalar or numpy array of any size\n",
    "    Returns:\n",
    "        y (scalar/ndarray): logistic function applied to x, has the same shape as x\n",
    "    \"\"\"\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, y_dash):\n",
    "    \"\"\"\n",
    "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
    "    Args:\n",
    "      y      (scalar): true value (0 or 1)\n",
    "      y_dash (scalar): predicted value (probability of y being 1)\n",
    "    Returns:\n",
    "      loss (float): nonnegative loss corresponding to y and y_dash\n",
    "    \"\"\"\n",
    "    loss = - (y * np.log(y_dash)) - ((1 - y) * np.log(1 - y_dash))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func_vec(y, y_dash):\n",
    "    \"\"\"\n",
    "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
    "    Args:\n",
    "      y      (array_like, shape (m,)): array of true values (0 or 1)\n",
    "      y_dash (array_like, shape (m,)): array of predicted values (probability of y being 1)\n",
    "    Returns:\n",
    "      cost (float): nonnegative cost corresponding to y and y_dash\n",
    "    \"\"\"\n",
    "    assert len(y) == len(y_dash), \"Length of true values and length of predicted values do not match\"\n",
    "    m = len(y)\n",
    "    loss_vec = np.array([log_loss(y[i], y_dash[i]) for i in range(m)])\n",
    "    cost = np.dot(loss_vec, np.ones(m)) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_logreg_vec(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function, given data and model parameters\n",
    "    Args:\n",
    "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
    "      y (array_like, shape (m,)): array of true values of target (0 or 1)\n",
    "      w (array_like, shape (n,)): weight parameters of the model      \n",
    "      b (float)                 : bias parameter of the model\n",
    "    Returns:\n",
    "      cost (float): nonnegative cost corresponding to y and y_dash \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
    "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
    "    z = np.matmul(X, w) + (b * np.ones(m))\n",
    "    y_dash = logistic(z)\n",
    "    cost = cost_func_vec(y, y_dash)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logreg_vec(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes gradients of the cost function with respect to model parameters\n",
    "    Args:\n",
    "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
    "      y (array_like, shape (m,)): array of true values of target (0 or 1)\n",
    "      w (array_like, shape (n,)): weight parameters of the model      \n",
    "      b (float)                 : bias parameter of the model\n",
    "    Returns:\n",
    "      grad_w (array_like, shape (n,)): gradients of the cost function with respect to the weight parameters\n",
    "      grad_b (float)                 : gradient of the cost function with respect to the bias parameter\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
    "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
    "    y_dash = logistic(np.matmul(X, w) + b * np.ones(m))\n",
    "    grad_w = np.matmul(y_dash - y, X) / m\n",
    "    grad_b = np.dot(y_dash - y, np.ones(m)) / m\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_logreg_vec_reg(X, y, w, b, l):\n",
    "    \"\"\"\n",
    "    Computes the cost function, given data and model parameters\n",
    "    Args:\n",
    "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
    "      y (array_like, shape (m,)): array of true values (0 or 1) of target\n",
    "      w (array_like, shape (n,)): weight parameters of the model      \n",
    "      b (float)                 : bias parameter of the model\n",
    "      l (float)                 : regularization parameter\n",
    "    Returns:\n",
    "      cost (float): nonnegative cost corresponding to y and y_dash \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
    "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
    "    cost = cost_logreg_vec(X, y, w, b)\n",
    "    cost += (l / (2 * m)) * np.dot(w, w)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logreg_vec_reg(X, y, w, b, l):\n",
    "    \"\"\"\n",
    "    Computes gradients of the cost function with respect to model parameters\n",
    "    Args:\n",
    "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
    "      y (array_like, shape (m,)): array of true values of target (0 or 1)\n",
    "      w (array_like, shape (n,)): weight parameters of the model      \n",
    "      b (float)                 : bias parameter of the model\n",
    "      l (float)                 : regularization parameter\n",
    "    Returns:\n",
    "      grad_w (array_like, shape (n,)): gradients of the cost function with respect to the weight parameters\n",
    "      grad_b (float)                 : gradient of the cost function with respect to the bias parameter\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
    "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
    "    grad_w, grad_b = grad_logreg_vec(X, y, w, b)\n",
    "    grad_w += (l / m) * w\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X, y, batch_size):\n",
    "  mini_batches = []\n",
    "  n_minibatches = X.shape[0]\n",
    "  i = 0\n",
    "  for i in range(n_minibatches + 1):\n",
    "    X_mini = X[i * batch_size:(i + 1)*batch_size, :]\n",
    "    Y_mini = y[i * batch_size:(i + 1)*batch_size]\n",
    "    if X_mini.shape[0] == 0:\n",
    "      break\n",
    "    mini_batches.append((X_mini, Y_mini))\n",
    "  return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hsv_train, X_hsv_val, y_hsv_train, y_hsv_val, = train_test_split(train_hsv_feature,\n",
    "                                                                    train_labels,\n",
    "                                                                      test_size=0.1,\n",
    "                                                                        random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_reg_with_mini_batch(X, y, w, b, l, alpha, n_iter, show_cost = True, show_params = False, batch_size=32): \n",
    "    \"\"\"\n",
    "    Implements batch gradient descent algorithm to learn and update model parameters\n",
    "    with prespecified number of interations and learning rate\n",
    "    Args:\n",
    "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
    "      y (array_like, shape (m,)): true values of target (0 or 1)\n",
    "      w (array_like, shape (n,)): initial value of weight parameters\n",
    "      b (scalar)                : initial value of bias parameter\n",
    "      l (float)                 : regularization parameter\n",
    "      alpha (float)             : learning rate\n",
    "      n_iter (int)              : number of iterations\n",
    "    Returns:\n",
    "      w (array_like, shape (n,)): updated values of weight parameters\n",
    "      b (scalar)                : updated value of bias parameter\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
    "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
    "    cost_history, params_history = [], []\n",
    "    for i, j in itertools.product(range(n_iter), range(1)):\n",
    "        mini_batches = create_mini_batches(X,y,batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "          X_mini, y_mini = mini_batch\n",
    "          grad_w, grad_b = grad_logreg_vec_reg(X_mini, y_mini, w, b, l)   \n",
    "          w += - alpha * grad_w\n",
    "          b += - alpha * grad_b\n",
    "          cost =  cost_logreg_vec_reg(X_mini, y_mini, w, b, l)\n",
    "          cost_history.append(cost)\n",
    "          params_history.append([w, b])\n",
    "\n",
    "        if show_cost == True and show_params == False and (i % math.ceil(n_iter / 10) == 0 or i == n_iter - 1):\n",
    "            print(f\"Iteration {i:6}:    Cost  {float(cost_history[i]):3.4f}\")\n",
    "        if show_cost == True and show_params == True and (i % math.ceil(n_iter / 10) == 0 or i == n_iter - 1):\n",
    "            print(f\"Iteration {i:6}:    Cost  {float(cost_history[i]):3.4f},    Params  {params_history[i]}\")\n",
    "    return w, b, cost_history, params_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_reg(X, y, w, b, l, alpha, n_iter, show_cost = True, show_params = False): \n",
    "    \"\"\"\n",
    "    Implements batch gradient descent algorithm to learn and update model parameters\n",
    "    with prespecified number of interations and learning rate\n",
    "    Args:\n",
    "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
    "      y (array_like, shape (m,)): true values of target (0 or 1)\n",
    "      w (array_like, shape (n,)): initial value of weight parameters\n",
    "      b (scalar)                : initial value of bias parameter\n",
    "      l (float)                 : regularization parameter\n",
    "      alpha (float)             : learning rate\n",
    "      n_iter (int)              : number of iterations\n",
    "    Returns:\n",
    "      w (array_like, shape (n,)): updated values of weight parameters\n",
    "      b (scalar)                : updated value of bias parameter\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
    "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
    "    cost_history, params_history = [], []\n",
    "    for i, j in itertools.product(range(n_iter), range(1)):\n",
    "        grad_w, grad_b = grad_logreg_vec_reg(X, y, w, b, l)   \n",
    "        w += - alpha * grad_w\n",
    "        b += - alpha * grad_b\n",
    "        cost =  cost_logreg_vec_reg(X, y, w, b, l)\n",
    "        cost_history.append(cost)\n",
    "        params_history.append([w, b])\n",
    "        if show_cost == True and show_params == False and (i % math.ceil(n_iter / 10) == 0 or i == n_iter - 1):\n",
    "            print(f\"Iteration {i:6}:    Cost  {float(cost_history[i]):3.4f}\")\n",
    "        if show_cost == True and show_params == True and (i % math.ceil(n_iter / 10) == 0 or i == n_iter - 1):\n",
    "            print(f\"Iteration {i:6}:    Cost  {float(cost_history[i]):3.4f},    Params  {params_history[i]}\")\n",
    "    return w, b, cost_history, params_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values of the model parameters\n",
    "# Initial values of the model parameters\n",
    "def create_weight():\n",
    "    w_init = np.zeros(train_hsv_feature.shape[1]).astype(float)\n",
    "    b_init = -1.\n",
    "    return w_init, b_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init, b_init = create_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/4000 [00:00<05:43, 11.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration      0:    Cost  0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 404/4000 [00:30<04:10, 14.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    400:    Cost  0.6815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 804/4000 [00:58<03:28, 15.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    800:    Cost  0.6553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1204/4000 [01:25<03:06, 14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   1200:    Cost  0.6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1604/4000 [01:50<02:22, 16.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   1600:    Cost  0.6424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2002/4000 [02:20<02:43, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   2000:    Cost  0.6508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 2403/4000 [02:45<01:35, 16.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   2400:    Cost  0.6528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2804/4000 [03:15<01:09, 17.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   2800:    Cost  0.6444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 3204/4000 [03:41<00:52, 15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   3200:    Cost  0.6815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 3604/4000 [04:08<00:24, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   3600:    Cost  0.6611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [04:32<00:00, 14.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   3999:    Cost  0.6611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning model parameters using gradient descent algorithm\n",
    "w_out_reg, b_out_reg, cost_history_reg, params_history_reg = grad_desc_reg_with_mini_batch(X_hsv_train,\n",
    "                                                                           y_hsv_train,\n",
    "                                                                           w = w_init, # np.zeros(X_train.shape[1]),\n",
    "                                                                           b = b_init, # 0,\n",
    "                                                                           l = 1.,\n",
    "                                                                           alpha = 0.1,\n",
    "                                                                           n_iter = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prob_reg = logistic(np.matmul(X_hsv_val, w_out_reg) + (b_out_reg * np.ones(X_hsv_val.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prob_reg = logistic(np.matmul(test_hsv_feature, w_out_reg) + (b_out_reg * np.ones(test_hsv_feature.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prob_reg =  (y_test_prob_reg > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, w, b):\n",
    "    y_prob = logistic(np.matmul(X, w) + (b * np.ones(X.shape[0])))\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_reg =  (y_train_prob_reg > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = hypothesis(X_hsv_val, w_out_reg, b_out_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_pred = accuracy_score(y_hsv_val, y_val_pred)\n",
    "print(\"Accuracy test:\", acc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = hypothesis(test_hsv_feature, w_out_reg, b_out_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 0.6866666666666666\n"
     ]
    }
   ],
   "source": [
    "acc_pred = accuracy_score(test_labels, y_test_pred)\n",
    "print(\"Accuracy test:\", acc_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel_py2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
